<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>GENEA Challenge 2022</title>

  <!-- Bootstrap core CSS -->
  <link href="/2022/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet">
  <link href="/2022/vendor/fontawesome-free/css/all.min.css" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="/2022/css/iva.min.css" rel="stylesheet">

</head>

<body id="page-top">
  <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
    <a class="navbar-brand js-scroll-trigger" href="#page-top">
      <span class="d-block d-lg-none">GENEA Challenge 2022</span>
      <!-- <span class="d-none d-lg-block">
        <img src="img/avatar.png" class="img-fluid img-profile rounded-circle mx-auto mb-5" alt="">

      </span> -->
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#home">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#important-dates">Important dates</a>
        </li>
        <!--
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#workshop-programme">Workshop programme</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#call-for-papers">Call for papers</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#reproducibility-award">Reproducibility Award</a>
        </li>

        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#invited-speakers">Invited speakers</a>
        </li>-->
        <!--
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#accepted-papers">Accepted papers</a>
        </li>-->
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#organising-committee">Organising committee</a>
        </li>
        <!--
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#program-committee">Program committee</a>
        </li>-->

      </ul>
    </div>
  </nav>

  <div class="container-fluid p-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="home">
      <div class="w-100">
        <div class="row">
          <div class="col-md-9 col-sm-12">
            <h1 class="mb-2">GENEA Challenge 2022
            </h1>

            <div class="subheading mb-5">Generation and Evaluation of Non-verbal Behaviour for Embodied Agents</div>
            <!--  -->

            <p class="mb-5">
              <b>Official <a href="https://icmi.acm.org/2022/grand-challenges/" target="_blank">ICMI 2022 Grand Challenge</a> – Start date May 16 (but register now!)</b><br><br>

              The <b>GENEA Challenge 2022 on speech-driven gesture generation</b> aims to bring together researchers that use different methods for non-verbal-behaviour generation and evaluation, and hopes to stimulate the discussions on how to improve both the generation methods and the evaluation of the results.<br><br>

              The results of the challenge will be presented at <a href="https://genea-workshop.github.io/2022/workshop/" target="_blank">the 3rd GENEA workshop </a> at <a href="https://icmi.acm.org/2022/" target="_blank">ACM ICMI 2022 </a>, with accepted challenge papers published in the main ICMI proceedings.<br><br>

              This is the second installment of the GENEA Challenge. You can <a href="https://svito-zar.github.io/GENEAchallenge2020/" target="_blank">read more about the previous GENEA Challenge here </a>.<br><br>

            </p>
            <div class="row justify-content-center">

              <img src="/2022/img/avatar.png"  class="img-fluid rounded" class="mt-2" width=300 alt="">
            </div>
          </div>
          <div class="col-md-3 d-none d-md-block">
            <a class="twitter-timeline" data-tweet-limit=3 href="https://twitter.com/WorkshopGenea?ref_src=twsrc%5Etfw" target="_blank">Follow us on twitter <i class="fab fa-twitter"></i></a>
            <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
          </div>
          <div class="col-xs-12 d-md-none mt-4 text-center w-100">
            <a href="https://twitter.com/WorkshopGenea" target="_blank">Follow us on Twitter <i class="fab fa-twitter"></i></a>
          </div>
        </div>
      </div>

    </section>

    <hr class="m-0">

    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="important-dates">
      <div class="w-100">
        <h2 class="mb-5">Important dates</h2>

       <div class="row">
          <div class="col">April 8, 2022</div>
          <div class="col">Participant registration opens</div>
       </div>
       
       <div class="row">
          <div class="col">May 16, 2022</div>
          <div class="col">challenge training dataset released to participants</div>
        </div>
        <div class="row">
          <div class="col">June 20, 2022</div>
          <div class="col">test input release</div>
        </div>
        <div class="row">
          <div class="col">June 27, 2022</div>
          <div class="col">deadline for participants to submit generated motion, start of evaluation</div>
        </div>
        <div class="row">
          <div class="col">July 11, 2022</div>
          <div class="col">release of crowdsourced evaluation results to participants</div>
        </div>
        <div class="row">
          <div class="col">July 20, 2022</div>
          <div class="col">paper submission deadline</div>
        </div>
	<div class="row">
          <div class="col">August 10, 2022</div>
          <div class="col">paper notification</div>
        </div>
        <div class="row">
          <div class="col">August 17, 2022</div>
          <div class="col">camera-ready version</div>
        </div>
        <div class="row">
          <div class="col">November 7 - 11, 2022</div>
          <div class="col">Challenge presentations</div>
        </div>

    </section>

    <hr class="m-0">

    <!--
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="workshop-programme">
      <div class="w-100">
        <h2 class="mb-5">Workshop programme</h2>
        <h4>Workshop programme:</h4>
        <p>TBD</p>
          </div>
        </div>
        <div class="row">
          <ul style="list-style: none">
            <li><span style="margin-right: 10px;">08:50</span> Wu et al. <a href="https://openreview.net/forum?id=ykvm7OLh7B" target="_blank">Probabilistic Human-like Gesture Synthesis from Speech using GRU-based WGAN</a></li>
            <li><span style="margin-right: 10px;">09:05</span> Schneeberger et al. <a href="https://openreview.net/forum?id=GjjPtEVdSLB" target="_blank">Influence of Movement Energy and Affect Priming on the Perception of Virtual Characters Extroversion and Mood</a></li>
            <li><span style="margin-right: 10px;">09:20</span> Lee et al. <a href="https://openreview.net/forum?id=o8CpxaBurZQ" target="_blank">Crossmodal clustered contrastive learning: Grounding of spoken language to gesture</a></li>
          </ul>
        </div>
        <div class="row">
          <div class="col">09:35&nbsp;-&nbsp;09:50</div>
          <div class="col-10">Break (Using gather.town)</div>
        </div>
        <div class="row">
          <div class="col">09:50&nbsp;-&nbsp;10:30</div>
          <div class="col-10"><a href="#louis-philippe_morency" class="js-scroll-trigger">Keynote 2 (Louis-Philippe Morency)</a></div>
        </div>
        <div class="row">
          <div class="col">10:30&nbsp;-&nbsp;10:45</div>
          <div class="col-10">Break (Using gather.town)</div>
        </div>
        <div class="row">
          <div class="col">10:45&nbsp;-&nbsp;10:50</div>
          <div class="col-10"><a href="#reproducibility-award" class="js-scroll-trigger">Reproducibility Award</a> announcement</div>
        </div>
        <div class="row">
          <div class="col">10:50&nbsp;-&nbsp;11:50</div>
          <div class="col-10">Group discussions</div>
        </div>
        <div class="row">
          <div class="col">11:50&nbsp;-&nbsp;11:55</div>
          <div class="col-10">Closing remarks</div>
        </div>
        <div class="row">
          <div class="col">12:00&nbsp;-&nbsp;</div>
          <div class="col-10">Informal mingle</div>
        </div>

      </div>
    </section>
    -->

    <!--
    <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="call-for-papers">-->
      <!-- justify-content-center -->
      <!--
      <div class="w-100">
        <h2 class="mb-5">Call for papers</h2>

        <div class="iva-item d-flex flex-column flex-md-row justify-content-between mb-5">
          <div class="iva-content">
            <h4>Overview</h4>
            Generating nonverbal behaviours, such as gesticulation, facial expressions and gaze, is of great importance
            for natural interaction with embodied agents such as virtual agents and social robots. At present, behaviour
            generation is typically powered by rule-based systems, data-driven approaches, and their hybrids. For
            evaluation, both objective and subjective methods exist, but their application and validity are frequently a
            point of contention.<br>
            <br>

            This workshop asks “What will be the behaviour-generation methods of the future? And how can we evaluate
            these methods using meaningful objective and subjective metrics?” The aim of the workshop is to bring
            together researchers working on the generation and evaluation of nonverbal behaviours for embodied agents to
            discuss the future of this field. To kickstart these discussions, we invite all interested researchers to
            submit a paper for presentation at the workshop.<br>
            <br>

            GENEA 2022 is the second GENEA workshop and an official workshop of ACM ICMI’22, which will take place
            either in Bangalore, India, or online. Accepted submissions will be included in the adjunct ACM ICMI
            proceedings.<br>
            <br>

            <h4>Paper topics include (but are not limited to) the following</h4>
            <ul>
              <li>Automated synthesis of facial expressions, gestures, and gaze movements</li>
              <li>Audio- and music-driven nonverbal behaviour synthesis</li>
              <li>Closed-loop nonverbal behaviour generation (from perception to action)</li>
              <li>Nonverbal behaviour synthesis in two-party and group interactions</li>
              <li>Emotion-driven and stylistic nonverbal behaviour synthesis</li>
              <li>New datasets related to nonverbal behaviour</li>
              <li>Believable nonverbal behaviour synthesis using motion-capture and 4D scan data</li>
              <li>Multi-modal nonverbal behaviour synthesis</li>
              <li>Interactive/autonomous nonverbal behavior generation</li>
              <li>Subjective and objective evaluation methods for nonverbal behaviour synthesis</li>
              <li>Guidelines for nonverbal behaviours in human-agent interaction</li>
            </ul>

            <h4>Submission guidelines</h4>
            The reviewing will be double blind, so submissions should be anonymous: do not include the authors' names, affiliations or any clearly identifiable information in the paper (including in the Acknowledgments and references). It is appropriate to cite past work of the authors if these citations are treated like any other (e.g., "Smith [5] approached this problem by....") - omit references only if it would be obviously identifying the authors. Paper chairs will desk reject non-anonymous papers after reviewing begins.
            <br><br>
            Submitted papers should conform to the latest ACM publication format. All authors should submit manuscripts for review in a double column format to ensure adherence to page limits. Please note that a non-anonymous author block may require a larger space than the anonymized version. For LaTeX templates and examples, please click on the following link: <a href="https://www.acm.org/publications/taps/word-template-workflow" target="_blank">https://www.acm.org/publications/taps/word-template-workflow</a>, download the zip package entitled Primary Article Template - LaTeX, and use the sample-sigconf.tex template with \documentclass[sigconf,review]{acmart} to add line numbers. We suggest you to use the LaTeX templates. You will find Word templates and examples on the same webpage mentioned. Authors who do decide to use the Word template should be made aware that an extra validation step may be required during the camera-ready process.
            <br><br>
            We will accept long (8 pages) and short (4 pages) paper submissions, along with posters (3 page papers), all
            in the double-column ACM conference format. Pages containing only references do not count toward the page
            limit for any of the paper types. Submissions should be made in PDF format through <a href="https://openreview.net/group?id=ACM.org/ICMI/2021/Workshop/GENEA" target="_blank">OpenReview</a>.
            <br><br>
            To encourage authors to make their work reproducible and reward the effort that this requires, we have introduced the <a class="js-scroll-trigger" href="#reproducibility-award">GENEA Workshop Reproducibility Award</a>.
          </div>
        </div>
      </div>
    </section>
  -->
    <!--<hr class="m-0">-->
    <!--
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="reproducibility-award">
    --><!-- justify-content-center -->
      <!--
      <div class="w-100">
        <h2 class="mb-5">Reproducibility Award</h2>
        Reproducibility is a cornerstone of the scientific method. Lack of reproducibility is a serious issue in contemporary research which we want to address at our workshop. To encourage authors to make their papers reproducible, and to reward the effort that reproducibility requires, we are introducing the GENEA Workshop Reproducibility Award. All short and long papers presented at the GENEA Workshop will be eligible for this award. Please note that it is the camera-ready version of the paper which will be evaluated for the reward.
        <br><br>
        The award is awarded to the paper with the greatest degree of reproducibility. The assessment criteria include:
        <ul>
          <li>ease of reproduction (ideal: just works, if there is code - it is well documented and we can run it)</li>
          <li>extent (ideal: all results can be verified)</li>
          <li>data accessibility (ideal: all data used is publicly available)</li>
        </ul>
        <div style="text-align: center; margin-top: 30px;">
          This year's award is awarded to: <b>Probabilistic Human-like Gesture Synthesis from Speech using GRU-based WGAN</b> <br> by
          <i>Bowen Wu, Chaoran Liu, Carlos Ishi, and Hiroshi Ishiguro</i>. <br><br>
          <img src="img/reproducibility_award_genea_2021.png" alt="reproducibility award" width=600>
        </div>
      </div>

    </section>-->
    <!--
    <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="invited-speakers">
      <div class="w-100">
        <h2 class="mb-5">Invited speakers</h2>

        <h3 class="mt-5" id="hatice_gunes"><a href="https://www.cl.cam.ac.uk/~hg410/">Hatice Gunes (University of Cambridge)</a></h3>
        <div class="row">
          <div class="col-2"><a href="https://www.cl.cam.ac.uk/~hg410/"> <img src="https://www.cl.cam.ac.uk/~hg410/HG2019.jpg"
            style=" margin-right: 30px; margin-bottom: 10px" class="img-fluid rounded" alt="Hatice Gunes"></a></div>
          <div class="col-10">
            <h5>Biography</h5>
            Hatice Gunes (Senior Member, IEEE) is a Professor with the Department of Computer Science and Technology, University of Cambridge, UK, leading the Affective Intelligence and Robotics (AFAR) Lab. Her expertise is in the areas of affective computing and social signal processing cross-fertilizing research in multimodal interaction, computer vision, signal processing, machine learning and social robotics. Dr Gunes’ team has published over 120 papers in these areas (citations > 5,700) and has received various awards and competitive grants, with funding from the Engineering and Physical Sciences Research Council UK (EPSRC), Innovate UK, British Council, and EU Horizon 2020. Dr Gunes is the former President of the Association for the Advancement of Affective Computing (2017-2019), the General Co-Chair of ACII 2019, and the Program Co-Chair of ACM/IEEE HRI 2020 and IEEE FG 2017. She was the Chair of the Steering Board of IEEE Transactions on Affective Computing in the period of 2017-2019. She has also served as an Associate Editor for IEEE Transactions on Affective Computing, IEEE Transactions on Multimedia, and Image and Vision Computing Journal. In 2019, Dr Gunes has been awarded the prestigious EPSRC Fellowship to investigate adaptive robotic emotional intelligence for wellbeing (2019-2024) and has been named a Faculty Fellow of the Alan Turing Institute – UK’s national centre for data science and artificial intelligence.
            <br>
            <br>
            <h5>Talk - Data-driven Robot Social Intelligence</h5>
            Designing artificially intelligent systems and interfaces with socio-emotional skills is a challenging task. Progress in industry and developments in academia provide us a positive outlook, however, the artificial social and emotional intelligence of the current technology is still limited. My lab’s research has been pushing the state of the art in a wide spectrum of research topics in this area, including the  design and creation of new datasets; novel feature representations and learning algorithms for sensing and understanding human nonverbal behaviours in solo, dyadic and group settings; and the data-driven generation of socially appropriate agent behaviours. In this talk, I will present some of my research team’s explorations in these areas, including audio-driven robot upper-body motion synthesis, and a dataset and a continual learning approach for assessing social appropriateness of robot actions.
        </div>
      </div>

        <h3 class="mt-5" id="louis-philippe_morency"><a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency (Carnegie Mellon University)</a></h3>
        <div class="row">
          <div class="col-2"><a href="https://www.cs.cmu.edu/~morency/"><img src="https://www.cs.cmu.edu/~morency/index_files/image001.jpg"
            style=" margin-right: 30px; margin-bottom: 10px" class="img-fluid rounded" alt="Louis-Philippe Morency"></a></div>
          <div class="col-10"><h5>Biography</h5>
            Louis-Philippe Morency is Associate Professor in the Language Technology Institute at Carnegie Mellon University where he leads the Multimodal Communication and Machine Learning Laboratory (MultiComp Lab). He was formerly research faculty in the Computer Sciences Department at University of Southern California and received his Ph.D. degree from MIT Computer Science and Artificial Intelligence Laboratory. His research focuses on building the computational foundations to enable computers with the abilities to analyze, recognize and predict subtle human communicative behaviors during social interactions. He received diverse awards including AI’s 10 to Watch by IEEE Intelligent Systems, NetExplo Award in partnership with UNESCO and 10 best paper awards at IEEE and ACM conferences. His research was covered by media outlets such as Wall Street Journal, The Economist and NPR.
            <br>
            <br>
            <h5>Talk - Multimodal AI: Learning Nonverbal Signatures</h5>
            Human face-to-face communication is a little like a dance, in that participants continuously adjust their behaviors based on verbal and nonverbal cues from the social context. Today's computers and interactive devices are still lacking many of these human-like abilities to hold fluid and natural interactions. One such capability is to generate natural-looking gestures when animating a virtual avatar or robot. In this talk, I will present some of our recent work towards learning nonverbal signatures of human speakers. Central to this research problem is the technical challenge of grounding, which links in this case spoken language with generated nonverbal gestures. This is one key step towards our longer-term vision of Multimodal AI, a family of technologies able to analyze, recognize and generate human subtle communicative behaviors in social context.
          </div>
        </div>
      </div>
    </section>

    <hr class="m-0">
  --><!---
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="accepted-papers">
      <div class="w-100">
        <h2 class="mb-5">Accepted papers</h2>

    </div>
    </section>
    --->



    <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="organising-committee">
      <div class="w-100">
        <h2 class="mb-5">Organising committee</h2>
        <p>
          The main contact address of the workshop is: <a
            href="mailto:genea-contact@googlegroups.com">genea-contact@googlegroups.com</a>. <br> <br>
          </p>
        <h4>Workshop organisers</h4>

        <div class="row">


          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="/2022/img/pieter.jpeg" width="100%" class="img-fluid rounded" alt="Pieter Wolfert">
              </div>
              <div class="col-7">
                  <a href="https://www.pieterwolfert.com" target="_blank" style="font-weight: bold;">Pieter Wolfert</a>
                  <br>
                  IDLab, Ghent University - imec <br> Belgium
              </div>
            </div>
            <hr>
          </div>

          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="/2022/img/taras.jpg" class="img-fluid rounded" alt="Taras Kucherenko">
              </div>
              <div class="col-7">
                <a href="https://svito-zar.github.io/" target="_blank" style="font-weight: bold;">Taras Kucherenko</a>
                <br>
                Electronic Arts (EA) <br> Sweden

              </div>
            </div>
            <hr>
          </div>
          

        </div>

        <div class="row">
          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="/2022/img/gustav.jpeg" class="img-fluid rounded" alt="Gustav Eje Henter">
              </div>
              <div class="col-7">

                <a href="https://people.kth.se/~ghe/" target="_blank" style="font-weight: bold;">Gustav Eje Henter</a>
                <br>
                KTH Royal Institute of Technology <br> Sweden


              </div>
            </div>
            <hr>
          </div>

         <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="/2022/img/zerrin.jpg" class="img-fluid rounded"  alt="Zerrin Yumak">
              </div>
              <div class="col-7">
                <a href="http://www.zerrinyumak.com" target="_blank" style="font-weight: bold;">Zerrin Yumak</a>
                <br>
                Utrecht University <br> The Netherlands
              </div>
            </div>
            <hr>
          </div>

        </div>

        <div class="row">
          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="/2022/img/youngwoo.jpg" class="img-fluid rounded" alt="Youngwoo Yoon">
              </div>
              <div class="col-7">

                <a href="https://sites.google.com/view/youngwoo-yoon/" target="_blank" style="font-weight: bold;">Youngwoo Yoon</a>
                <br>ETRI <br> South Korea


              </div>
            </div>
            <hr>
          </div>
          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="/2022/img/carla.jpg" class="img-fluid rounded" alt="Carla Viegas">
              </div>
              <div class="col-7">
                  <a href="https://www.carlaviegas.info" target="_blank" style="font-weight: bold;">Carla Viegas</a>
                  <br>
                  Carnegie Mellon University <br> United States of America
              </div>
            </div>
            <hr>
          </div>
        </div>

      </div>
    </section>
    <hr class="m-0">

    <!--
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="program-committee">
      <div class="w-100">
        <h2 class="mb-5">Program committee</h2>
        TBD
      </div>

    </section>
    <hr class="m-0">
    --->


  </div>

  <!-- Bootstrap core JavaScript -->
  <script src="/2022/vendor/jquery/jquery.min.js"></script>
  <script src="/2022/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="/2022/vendor/jquery-easing/jquery.easing.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="/2022/js/iva.min.js"></script>

</body>

</html>
